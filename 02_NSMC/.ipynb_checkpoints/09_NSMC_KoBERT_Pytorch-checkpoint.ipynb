{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 영화리뷰 감정분석 with SKT KoBERT\n",
    "참고 소스 출처(링크) : https://github.com/SKTBrain/KoBERT\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT#using-with-pytorch\n",
    "\n",
    "https://colab.research.google.com/github/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비\n",
    "라이브러리, 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mxnet\n",
    "# !pip install gluonnlp pandas tqdm\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "MAXLEN = 128  # 64\n",
    "BATCHSIZE = 32  # 64\n",
    "EPOCHS = 5  # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# 디바이스 설정 확인\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  2\n"
     ]
    }
   ],
   "source": [
    "# GPU 할당 변경하기\n",
    "GPU_NUM = 2 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoBERT Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 데이터 로드\n",
    "train = pd.read_csv(DATA_IN_PATH + 'ratings_train.txt', sep='\\t')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증(Original Test Dataset) 데이터 로드\n",
    "dev = pd.read_csv(DATA_IN_PATH + 'ratings_test.txt', sep='\\t')\n",
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>정말 많이 울었던 영화입니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>시간 낭비예요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>이걸 영화로 만드는 거야?얼마나 가는지 보자.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                document\n",
       "0   0                        정말 많이 울었던 영화입니다.\n",
       "1   1                                시간 낭비예요.\n",
       "2   2  포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다.\n",
       "3   3    지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!!\n",
       "4   4               이걸 영화로 만드는 거야?얼마나 가는지 보자."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트(캐글) 데이터 로드\n",
    "test = pd.read_csv(DATA_IN_PATH + 'ko_data.csv', encoding = 'cp949')\n",
    "test.columns = ['id','document']\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/검증 데이터를 txt 파일로 저장\n",
    "train.to_csv(DATA_IN_PATH+'train.txt', sep = '\\t', index = True)\n",
    "dev.to_csv(DATA_IN_PATH+'dev.txt', sep = '\\t', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트(캐글) 데이터를 txt 파일로 저장(label 없음)\n",
    "csv_file = './data_in/ko_data.csv'\n",
    "txt_file = './data_in/ko_data.txt'\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\", encoding='cp949') as my_input_file: # , encoding='cp949'\n",
    "        [ my_output_file.write(\"\\t\".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train= nlp.data.TSVDataset(DATA_IN_PATH+\"train.txt\", field_indices=[2,3], num_discard_samples=1)\n",
    "dataset_dev = nlp.data.TSVDataset(DATA_IN_PATH+\"dev.txt\", field_indices=[2,3], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = nlp.data.TSVDataset(DATA_IN_PATH+\"ko_data.txt\", field_indices=[1], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gluonnlp.data.dataset.TSVDataset at 0x7f62d895fdd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset_Test(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        #self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = MAXLEN  # 64\n",
    "batch_size = BATCHSIZE  # 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = EPOCHS  # 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_dev = BERTDataset(dataset_dev, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset_Test(dataset_test, 0, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "dev_dataloader = torch.utils.data.DataLoader(data_dev, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ea77858a7d4431b83533c2aa09c404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4688.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.8223128914833069 train acc 0.34375\n",
      "epoch 1 batch id 201 loss 0.6484859585762024 train acc 0.5212997512437811\n",
      "epoch 1 batch id 401 loss 0.5482144355773926 train acc 0.6188435162094763\n",
      "epoch 1 batch id 601 loss 0.3797118365764618 train acc 0.6797004991680532\n",
      "epoch 1 batch id 801 loss 0.48848333954811096 train acc 0.7158239700374532\n",
      "epoch 1 batch id 1001 loss 0.3856777846813202 train acc 0.7411963036963037\n",
      "epoch 1 batch id 1201 loss 0.4960686266422272 train acc 0.7584825145711906\n",
      "epoch 1 batch id 1401 loss 0.3125966787338257 train acc 0.7716809421841542\n",
      "epoch 1 batch id 1601 loss 0.5354685187339783 train acc 0.781816052467208\n",
      "epoch 1 batch id 1801 loss 0.4149617552757263 train acc 0.7892837312604108\n",
      "epoch 1 batch id 2001 loss 0.3206002116203308 train acc 0.7955553473263368\n",
      "epoch 1 batch id 2201 loss 0.8603451251983643 train acc 0.8009427532939573\n",
      "epoch 1 batch id 2401 loss 0.2964036762714386 train acc 0.8054066014160767\n",
      "epoch 1 batch id 2601 loss 0.2645609676837921 train acc 0.8094002306805075\n",
      "epoch 1 batch id 2801 loss 0.3323437571525574 train acc 0.8129239557300963\n",
      "epoch 1 batch id 3001 loss 0.31069415807724 train acc 0.8157697434188603\n",
      "epoch 1 batch id 3201 loss 0.2428991198539734 train acc 0.8184258825367072\n",
      "epoch 1 batch id 3401 loss 0.32731524109840393 train acc 0.821091223169656\n",
      "epoch 1 batch id 3601 loss 0.2524453103542328 train acc 0.8236774507081366\n",
      "epoch 1 batch id 3801 loss 0.34437721967697144 train acc 0.826427255985267\n",
      "epoch 1 batch id 4001 loss 0.23012793064117432 train acc 0.8289177705573607\n",
      "epoch 1 batch id 4201 loss 0.23374608159065247 train acc 0.831014936919781\n",
      "epoch 1 batch id 4401 loss 0.35532504320144653 train acc 0.8330990115882754\n",
      "epoch 1 batch id 4601 loss 0.24229997396469116 train acc 0.8350426537709194\n",
      "\n",
      "epoch 1 train acc 0.835837510665529\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5f3ccfea8f4e1fb73783b4cf256c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4688.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.40682393312454224 train acc 0.78125\n",
      "epoch 2 batch id 201 loss 0.27893537282943726 train acc 0.8826181592039801\n",
      "epoch 2 batch id 401 loss 0.28871193528175354 train acc 0.8817019950124688\n",
      "epoch 2 batch id 601 loss 0.2230505496263504 train acc 0.8811356073211315\n",
      "epoch 2 batch id 801 loss 0.21518996357917786 train acc 0.882334581772784\n",
      "epoch 2 batch id 1001 loss 0.3399496376514435 train acc 0.8837100399600399\n",
      "epoch 2 batch id 1201 loss 0.33655238151550293 train acc 0.8852258534554538\n",
      "epoch 2 batch id 1401 loss 0.3510580360889435 train acc 0.8860635260528195\n",
      "epoch 2 batch id 1601 loss 0.42435014247894287 train acc 0.8868480637101811\n",
      "epoch 2 batch id 1801 loss 0.2732967138290405 train acc 0.8877533314825097\n",
      "epoch 2 batch id 2001 loss 0.2490934580564499 train acc 0.8883214642678661\n",
      "epoch 2 batch id 2201 loss 0.3309955298900604 train acc 0.8891271013175829\n",
      "epoch 2 batch id 2401 loss 0.3931578993797302 train acc 0.8902800916284881\n",
      "epoch 2 batch id 2601 loss 0.03605201467871666 train acc 0.8908112264513649\n",
      "epoch 2 batch id 2801 loss 0.12008348852396011 train acc 0.891846661906462\n",
      "epoch 2 batch id 3001 loss 0.23147548735141754 train acc 0.8926503665444852\n",
      "epoch 2 batch id 3201 loss 0.2976621687412262 train acc 0.8935196032489847\n",
      "epoch 2 batch id 3401 loss 0.43228158354759216 train acc 0.8944244339900029\n",
      "epoch 2 batch id 3601 loss 0.08709308505058289 train acc 0.8956539850041655\n",
      "epoch 2 batch id 3801 loss 0.39211592078208923 train acc 0.8963266245724809\n",
      "epoch 2 batch id 4001 loss 0.11096492409706116 train acc 0.8974084603849037\n",
      "epoch 2 batch id 4201 loss 0.1836010068655014 train acc 0.8982087598190907\n",
      "epoch 2 batch id 4401 loss 0.2836116552352905 train acc 0.8991777436946149\n",
      "epoch 2 batch id 4601 loss 0.15367229282855988 train acc 0.9001779504455553\n",
      "\n",
      "epoch 2 train acc 0.9007039249146758\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a451056941cc4f0cb65a4d557a47528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4688.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.33729299902915955 train acc 0.84375\n",
      "epoch 3 batch id 201 loss 0.15499551594257355 train acc 0.919931592039801\n",
      "epoch 3 batch id 401 loss 0.124224953353405 train acc 0.9205891521197007\n",
      "epoch 3 batch id 601 loss 0.2853637933731079 train acc 0.9211730449251248\n",
      "epoch 3 batch id 801 loss 0.3366405665874481 train acc 0.9230259051186017\n",
      "epoch 3 batch id 1001 loss 0.28993335366249084 train acc 0.9242007992007992\n",
      "epoch 3 batch id 1201 loss 0.40696951746940613 train acc 0.9252966278101582\n",
      "epoch 3 batch id 1401 loss 0.3583436906337738 train acc 0.9261018915060671\n",
      "epoch 3 batch id 1601 loss 0.41202470660209656 train acc 0.9272134603372892\n",
      "epoch 3 batch id 1801 loss 0.15729005634784698 train acc 0.9286160466407551\n",
      "epoch 3 batch id 2001 loss 0.09297055751085281 train acc 0.9295508495752124\n",
      "epoch 3 batch id 2201 loss 0.41372087597846985 train acc 0.9304435483870968\n",
      "epoch 3 batch id 2401 loss 0.1043100506067276 train acc 0.9318122657226156\n",
      "epoch 3 batch id 2601 loss 0.07822263985872269 train acc 0.932742214532872\n",
      "epoch 3 batch id 2801 loss 0.017098519951105118 train acc 0.9332604426990361\n",
      "epoch 3 batch id 3001 loss 0.13993659615516663 train acc 0.9336679440186605\n",
      "epoch 3 batch id 3201 loss 0.21191881597042084 train acc 0.9345809903155264\n",
      "epoch 3 batch id 3401 loss 0.17841856181621552 train acc 0.9352855777712438\n",
      "epoch 3 batch id 3601 loss 0.07282204180955887 train acc 0.9359552901971675\n",
      "epoch 3 batch id 3801 loss 0.05329602211713791 train acc 0.9367271770586688\n",
      "epoch 3 batch id 4001 loss 0.03927018493413925 train acc 0.9374687578105474\n",
      "epoch 3 batch id 4201 loss 0.01961088553071022 train acc 0.9379909545346347\n",
      "epoch 3 batch id 4401 loss 0.17469270527362823 train acc 0.9386077027948193\n",
      "epoch 3 batch id 4601 loss 0.1831938773393631 train acc 0.9392387524451207\n",
      "\n",
      "epoch 3 train acc 0.9395797781569966\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff62dfe404442a38b285801e1c6763b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4688.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.32589760422706604 train acc 0.90625\n",
      "epoch 4 batch id 201 loss 0.08926602452993393 train acc 0.9508706467661692\n",
      "epoch 4 batch id 401 loss 0.04859985411167145 train acc 0.95160536159601\n",
      "epoch 4 batch id 601 loss 0.14075715839862823 train acc 0.9534109816971714\n",
      "epoch 4 batch id 801 loss 0.18673458695411682 train acc 0.9547440699126092\n",
      "epoch 4 batch id 1001 loss 0.04200735688209534 train acc 0.9551698301698301\n",
      "epoch 4 batch id 1201 loss 0.43951284885406494 train acc 0.955661948376353\n",
      "epoch 4 batch id 1401 loss 0.17713716626167297 train acc 0.9560581727337616\n",
      "epoch 4 batch id 1601 loss 0.4491548538208008 train acc 0.956648188632105\n",
      "epoch 4 batch id 1801 loss 0.12827111780643463 train acc 0.9571418656302054\n",
      "epoch 4 batch id 2001 loss 0.06795501708984375 train acc 0.9580990754622689\n",
      "epoch 4 batch id 2201 loss 0.3580518662929535 train acc 0.9588681281235802\n",
      "epoch 4 batch id 2401 loss 0.013877208344638348 train acc 0.9598604748021657\n",
      "epoch 4 batch id 2601 loss 0.005834817420691252 train acc 0.9602196270665129\n",
      "epoch 4 batch id 2801 loss 0.005480040330439806 train acc 0.9603378257765084\n",
      "epoch 4 batch id 3001 loss 0.1153680831193924 train acc 0.9607110129956681\n",
      "epoch 4 batch id 3201 loss 0.06303831189870834 train acc 0.9612621055920025\n",
      "epoch 4 batch id 3401 loss 0.20414096117019653 train acc 0.9617024404586886\n",
      "epoch 4 batch id 3601 loss 0.0162111297249794 train acc 0.9622761038600389\n",
      "epoch 4 batch id 3801 loss 0.11143036186695099 train acc 0.9624769797421732\n",
      "epoch 4 batch id 4001 loss 0.009164017625153065 train acc 0.9629701949512622\n",
      "epoch 4 batch id 4201 loss 0.033485423773527145 train acc 0.9631337776719828\n",
      "epoch 4 batch id 4401 loss 0.06984448432922363 train acc 0.9633534992047262\n",
      "epoch 4 batch id 4601 loss 0.02356533706188202 train acc 0.9639684307759183\n",
      "\n",
      "epoch 4 train acc 0.9642304820819113\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c1b94c1b854273bf85dbd2f6e49ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4688.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.28779685497283936 train acc 0.90625\n",
      "epoch 5 batch id 201 loss 0.04054595157504082 train acc 0.9715485074626866\n",
      "epoch 5 batch id 401 loss 0.03454947844147682 train acc 0.9732699501246883\n",
      "epoch 5 batch id 601 loss 0.019890131428837776 train acc 0.9740536605657238\n",
      "epoch 5 batch id 801 loss 0.008477913215756416 train acc 0.9744460049937578\n",
      "epoch 5 batch id 1001 loss 0.005957565736025572 train acc 0.9748688811188811\n",
      "epoch 5 batch id 1201 loss 0.26583659648895264 train acc 0.9744743963363863\n",
      "epoch 5 batch id 1401 loss 0.09290117770433426 train acc 0.9750624553890078\n",
      "epoch 5 batch id 1601 loss 0.22208064794540405 train acc 0.9754645534041224\n",
      "epoch 5 batch id 1801 loss 0.11809958517551422 train acc 0.9757599944475291\n",
      "epoch 5 batch id 2001 loss 0.004006384406238794 train acc 0.9760588455772113\n",
      "epoch 5 batch id 2201 loss 0.24160245060920715 train acc 0.9764311676510677\n",
      "epoch 5 batch id 2401 loss 0.0084375673905015 train acc 0.9768586005830904\n",
      "epoch 5 batch id 2601 loss 0.0039327251724898815 train acc 0.9770160515186467\n",
      "epoch 5 batch id 2801 loss 0.00700870668515563 train acc 0.9770729203855766\n",
      "epoch 5 batch id 3001 loss 0.01184331625699997 train acc 0.977236754415195\n",
      "epoch 5 batch id 3201 loss 0.009061471559107304 train acc 0.9774777413308341\n",
      "epoch 5 batch id 3401 loss 0.10523585975170135 train acc 0.9776536312849162\n",
      "epoch 5 batch id 3601 loss 0.0038794942665845156 train acc 0.9778360177728409\n",
      "epoch 5 batch id 3801 loss 0.00454059150069952 train acc 0.9778923309655354\n",
      "epoch 5 batch id 4001 loss 0.006704345811158419 train acc 0.9781070357410647\n",
      "epoch 5 batch id 4201 loss 0.004750255029648542 train acc 0.978093013568198\n",
      "epoch 5 batch id 4401 loss 0.2204054743051529 train acc 0.9782009770506703\n",
      "epoch 5 batch id 4601 loss 0.007884301245212555 train acc 0.9784014344707672\n",
      "\n",
      "epoch 5 train acc 0.9784956271331058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    print()\n",
    "    print('======== Epoch {:} / {:} ========'.format(e+1, num_epochs))\n",
    "    \n",
    "    # 모델 학습\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "#     # 모델 평가 => GPU 메모리 최적화 위해 주석 처리\n",
    "#     model.eval()\n",
    "#     for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(dev_dataloader)):\n",
    "#         token_ids = token_ids.long().to(device)\n",
    "#         segment_ids = segment_ids.long().to(device)\n",
    "#         valid_length= valid_length\n",
    "#         label = label.long().to(device)\n",
    "#         out = model(token_ids, valid_length, segment_ids)\n",
    "#         test_acc += calc_accuracy(out, label)\n",
    "#     print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c297fb2ad24ff4b151fabc41ca335d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1563.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_preds = []\n",
    "y_dev = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(dev_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length = valid_length\n",
    "    label = label.to('cpu').numpy()  # label을 torch tensor에서 numpy 타입으로 변경\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    \n",
    "    for i in label:\n",
    "        y_dev.append(i)\n",
    "    \n",
    "    for i in range(len(out)):\n",
    "        dev_preds.append(np.argmax(out[i].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 0, 0, 0], [1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_preds[:5], y_dev[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score : 0.89658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_dev, dev_preds, average='weighted')\n",
    "print('F1 Score : {:.5f}'.format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (참고) 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7249ea7e784d65b3746b49b3daef40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length = valid_length\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    \n",
    "    for i in range(len(out)):\n",
    "        test_preds.append(np.argmax(out[i].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11187"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Predicted\n",
       "0   0          1\n",
       "1   1          0\n",
       "2   2          1\n",
       "3   3          1\n",
       "4   4          0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터의 리뷰 부분을 리스트 처리\n",
    "test_id = list(test['id'])\n",
    "\n",
    "# 데이터프레임 통해 데이터 구성하여 output에 투입\n",
    "output = pd.DataFrame( data={\"Id\": test_id, \"Predicted\": test_preds} )\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 경로가 없으면 생성\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "SAVE_NM = \"NSMC_KoBERT_MAXLN\"+str(max_len)+\"_BATSZ\"+str(batch_size)+\"_EPOCH\"+str(num_epochs)+\".csv\"\n",
    "\n",
    "# csv 파일 생성\n",
    "output.to_csv(DATA_OUT_PATH + SAVE_NM, index = False)  # 앙상블 조합 및 캐글 제출 용도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐글 제출 결과\n",
    "**[2020.12.14]**<br>\n",
    "max_len = 64, batch_size = 64, epoch 1 => 0.87162<br>\n",
    "max_len = 64, batch_size = 64, epoch 3 => 0.89719<br>\n",
    "**max_len = 64, batch_size = 64, epoch 5 => 0.90076** => 0.90452<br>\n",
    "\n",
    "max_len = 64, batch_size = 32, epoch 1 => 0.87681<br>\n",
    "max_len = 64, batch_size = 32, epoch 3 => 0.88896<br>\n",
    "**max_len = 64, batch_size = 32, epoch 5 => 0.90005** => 0.90059<br>\n",
    "\n",
    "max_len = 128, batch_size = 32, epoch 1 => 0.87806<br>\n",
    "max_len = 128, batch_size = 32, epoch 3 => 0.89504<br>\n",
    "**max_len = 128, batch_size = 32, epoch 5 => 0.90112** => 0.90541<br>\n",
    "\n",
    "(실패) max_len = 128, batch_size = 64, epoch 1 => Out of Memory (GPU)<br>\n",
    "(실패) max_len = 128, batch_size = 64, epoch 3 => Out of Memory (GPU)<br>\n",
    "(실패) max_len = 128, batch_size = 64, epoch 5 => Out of Memory (GPU)<br>\n",
    "\n",
    "**[2020.12.15]**<br>\n",
    "**max_len = 64, batch_size = 32, epoch 5 => 0.90059<br>\n",
    "max_len = 64, batch_size = 64, epoch 5 => 0.90452<br>\n",
    "max_len = 128, batch_size = 32, epoch 5 => 0.90541**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
